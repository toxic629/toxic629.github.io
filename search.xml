<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>ResNetcode</title>
      <link href="/2025/10/31/ResNetcode/"/>
      <url>/2025/10/31/ResNetcode/</url>
      
        <content type="html"><![CDATA[<h1 id="回顾"><a href="#回顾" class="headerlink" title="回顾"></a>回顾</h1><p>　　经过ResNet的网络框架学习之后，现在来开始代码的实现</p><h1 id="残差块的搭建"><a href="#残差块的搭建" class="headerlink" title="残差块的搭建"></a>残差块的搭建</h1><p>首先定义一个残差块，每一步骤都在注释上写了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Residual</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_channels, num_channels, use_1conv=<span class="literal">False</span>, strides=<span class="number">1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Residual, <span class="variable language_">self</span>).__init__() <span class="comment"># 初始化</span></span><br><span class="line">        <span class="variable language_">self</span>.ReLU = nn.ReLU() <span class="comment"># 定义ReLU激活函数</span></span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(in_channels=input_channels, out_channels=num_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=strides) <span class="comment">#定义第一个卷积，卷积核大小为3 填充为1 步幅为1</span></span><br><span class="line">        <span class="variable language_">self</span>.conv2 = nn.Conv2d(in_channels=num_channels,  out_channels=num_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.bn1 = nn.BatchNorm2d(num_channels) <span class="comment"># BatchNormal归一化</span></span><br><span class="line">        <span class="variable language_">self</span>.bn2 = nn.BatchNorm2d(num_channels)</span><br><span class="line">        <span class="keyword">if</span> use_1conv: <span class="comment"># 如果有1*1的卷积块</span></span><br><span class="line">            <span class="variable language_">self</span>.conv3 = nn.Conv2d(in_channels=input_channels, out_channels=num_channels, kernel_size=<span class="number">1</span>, stride=strides)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.conv3 = <span class="literal">None</span> <span class="comment"># 没有就定义为空</span></span><br></pre></td></tr></table></figure><h2 id="残差块的前向传播"><a href="#残差块的前向传播" class="headerlink" title="残差块的前向传播"></a>残差块的前向传播</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">    y = <span class="variable language_">self</span>.ReLU(<span class="variable language_">self</span>.bn1(<span class="variable language_">self</span>.conv1(x)))<span class="comment">#先卷积，再BN层，再ReLu激活函数</span></span><br><span class="line">    y = <span class="variable language_">self</span>.bn2(<span class="variable language_">self</span>.conv2(y)) <span class="comment">#第二层卷积再经过 BN层</span></span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.conv3: <span class="comment">#跳转连接</span></span><br><span class="line">        x = <span class="variable language_">self</span>.conv3(x)</span><br><span class="line">    y = <span class="variable language_">self</span>.ReLU(y+x)</span><br><span class="line">    <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure><h1 id="ResNet-18整体网络的搭建"><a href="#ResNet-18整体网络的搭建" class="headerlink" title="ResNet-18整体网络的搭建"></a>ResNet-18整体网络的搭建</h1><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ResNet18</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, Residual</span>):</span><br><span class="line">        <span class="built_in">super</span>(ResNet18, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.b1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">            nn.ReLU(),<span class="comment">#BN层在激活函数之前效果会更好</span></span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"><span class="comment">#下面是进行传播的代码</span></span><br><span class="line">        <span class="variable language_">self</span>.b2 = nn.Sequential(Residual(<span class="number">64</span>, <span class="number">64</span>, use_1conv=<span class="literal">False</span>, strides=<span class="number">1</span>),</span><br><span class="line">                                Residual(<span class="number">64</span>, <span class="number">64</span>, use_1conv=<span class="literal">False</span>, strides=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.b3 = nn.Sequential(Residual(<span class="number">64</span>, <span class="number">128</span>, use_1conv=<span class="literal">True</span>, strides=<span class="number">2</span>),</span><br><span class="line">                                Residual(<span class="number">128</span>, <span class="number">128</span>, use_1conv=<span class="literal">False</span>, strides=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.b4 = nn.Sequential(Residual(<span class="number">128</span>, <span class="number">256</span>, use_1conv=<span class="literal">True</span>, strides=<span class="number">2</span>),</span><br><span class="line">                                Residual(<span class="number">256</span>, <span class="number">256</span>, use_1conv=<span class="literal">False</span>, strides=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.b5 = nn.Sequential(Residual(<span class="number">256</span>, <span class="number">512</span>, use_1conv=<span class="literal">True</span>, strides=<span class="number">2</span>),</span><br><span class="line">                                Residual(<span class="number">512</span>, <span class="number">512</span>, use_1conv=<span class="literal">False</span>, strides=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.b6 = nn.Sequential(nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>)), <span class="comment">#全局平均池化</span></span><br><span class="line">                                nn.Flatten(), <span class="comment">#平展层</span></span><br><span class="line">                                nn.Linear(<span class="number">512</span>, <span class="number">10</span>)) <span class="comment">#全连接 10代表分类数目</span></span><br></pre></td></tr></table></figure><h1 id="网络整体的前向传播"><a href="#网络整体的前向传播" class="headerlink" title="网络整体的前向传播"></a>网络整体的前向传播</h1><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">    x = <span class="variable language_">self</span>.b1(x)</span><br><span class="line">    x = <span class="variable language_">self</span>.b2(x)</span><br><span class="line">    x = <span class="variable language_">self</span>.b3(x)</span><br><span class="line">    x = <span class="variable language_">self</span>.b4(x)</span><br><span class="line">    x = <span class="variable language_">self</span>.b5(x)</span><br><span class="line">    x = <span class="variable language_">self</span>.b6(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>ResNet</title>
      <link href="/2025/10/30/ResNet/"/>
      <url>/2025/10/30/ResNet/</url>
      
        <content type="html"><![CDATA[<h1 id="前情提要"><a href="#前情提要" class="headerlink" title="前情提要"></a>前情提要</h1><p>　　已经学过了LeNet,AlexNet,VGG,GoogleNet等深度学习的基本模型,在ResNet,没发现之前,深度学习模型为了提升性能都会去增加深度,但无脑的增加深度就会引起梯度的爆炸或消失.为了解决这个主要问题,ResNet模型应运而生.而且学习ResNet模型也为后续的学习打下基础(后续模型有很多结构用了ResNet)</p><h1 id="ResNet的诞生背景"><a href="#ResNet的诞生背景" class="headerlink" title="ResNet的诞生背景"></a>ResNet的诞生背景</h1><p>​         深度残差网络(deep residual network)是2015年微软何凯明团队发表的一篇名为：《Deep Residual Learning for Image Recognition》的论文中提出的一种全新的网络结构，其核心模块是残差块residual block。正是由于残差块结构的出现使得深度神经网络模型的层数可以不断加深到100层、1000层甚至更深，从而使得该团队在当年的ILSVRC 2015分类竞赛中取得卓越成绩，也深刻地影响了以后的很多深度神经网络的结构设计。 残差网络的成功不仅表现在其在ILSVRC 2015竞赛中的卓越效果，更是因为残差块skipconnection&#x2F;shorcut这样优秀的思想和设计，使得卷积网络随着层数加深而导致的模型退化问题能被够大幅解决，使模型深度提高一个数量级，到达上百、上千层。</p><h1 id="ResNet的基础架构–残差块"><a href="#ResNet的基础架构–残差块" class="headerlink" title="ResNet的基础架构–残差块"></a>ResNet的基础架构–残差块</h1><p><img src="091516.png" alt="残差块"></p><p> 　　ResNet的基础架构–残差块（residual block）。在残差块中，输入可通过跨层数据线路更快地向前传播。ResNet沿用了VGG完整的3×3卷积层设计。残差块里首先有2个有相同输出通道数的3×3卷积层。每个卷积层后接一个批量规范化层和ReLU激活函数。然后我们通过跨层数据通路，跳过这2个卷积运算，将输入直接加在最后的ReLU激活函数前。这样的设计要求2个卷积层的输出与输入形状一样，从而使它们可以相加。如果想改变通道数，就需要引入一个额外的1×1卷积层来将输入变换成需要的形状后再做相加运算。</p><h2 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h2><ol><li><p>输入为224×224×3，卷积核数量为64个；卷积核的尺寸大小为3×3×3；步幅为1（stride&#x3D;1），填充为1（padding&#x3D;1）；卷积后得到shape为224×224×64的特征图输出。</p></li><li><p>输入为224×224×64，将输入的特征图经过批量规范化，然后经过RULE激活函数进行激活。输出的特征图大小形状不变，为224×224×64。</p></li><li><p>输入为224×224×64，卷积核数量为3个；卷积核的尺寸大小为3×3×64；步幅为1（stride&#x3D;1），填充为1（padding&#x3D;1）；卷积后得到shape为224×224×3的特征图输出。</p></li><li><p>输入为224×224×3，将输入的特征图经过批量规范化。输出的特征图大小形状不变，为224×224×3。</p></li><li><p>此时，残差结构需要输出特征图之前将经过上述（1）、（2）、（3）、（4）操作的输出特征图和一开始的输入x进行</p><p>相加操作；因为x的大小为224×224×3，经过（1）、（2）、（3）、（4）操作的输出特征图大小为224×224×3；这两个特征图的高宽和通道数一样，因此此时就不需要利用1×1的卷积核对输出x的通道进行改变，可以直接进行相加即可。</p></li></ol><h1 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h1><p>　　BN是由Google于2015年提出，这是一个深度神经训练的技巧，它不仅可以加快了模型的收敛速度，使训练深层网络模型更加容易和稳定。目前BN已经成为几乎所有卷积神经网络的标配技巧了。从字面意思看来Batch Normalization（简称BN）就是对每一批数据进行归一化，确实如此，对于训练中某一个batch的数据{x1,x2,…,xn}，注意这个数据是可以输入也可以是网络中间的某一层输出。</p><p>　　从字面意思看来Batch Normalization（简称BN）就是对每一批数据进行归一化，确实如此，对于训练中某一个batch的数据{x1,x2,…,xn}，注意这个数据是可以输入也可以是网络中间的某一层输出。在BN出现之前，我们的归一化操作一般都在数据输入层，对输入的数据进行求均值以及求方差做归一化，但是BN的出现打破了这一个规定，我们可以在网络中任意一层进行归一化处理，因为我们现在所用的优化方法大多都是min-batch SGD，所以我们的归一化操作就成为Batch Normalization。</p><h2 id="BN的步骤"><a href="#BN的步骤" class="headerlink" title="BN的步骤"></a>BN的步骤</h2><p><img src="100326.png" alt="过程"></p><p>如上图所示，BN步骤主要分为4步：</p><ol><li><p>求每一个训练批次数据的均值</p></li><li><p>求每一个训练批次数据的方差</p></li><li><p>使用求得的均值和方差对该批次的训练数据做归一化，获得0-1分布。其中ε是为了避免除数为0时所使用的微小正数。</p></li><li><p>尺度变换和偏移：将xi乘以γ调整数值大小，再加上β增加偏移后得到yi，这里的γ是尺度因子，β是平移因子。这一步是BN的精髓，由于归一化后的xi基本会被限制在正态分布下，使得网络的表达能力下降。为解决该问题，我们引入两个新的参数： γ,β。γ和β是在训练时网络自己学习得到的。</p></li></ol><h2 id="BN的总结"><a href="#BN的总结" class="headerlink" title="BN的总结"></a>BN的总结</h2><ol><li><p>加速收敛</p></li><li><p>解决梯度消失和梯度爆</p></li><li><p>可以不需要小心翼翼地设置权重初始化初始化对学习的影响减小了，可以不那么小心地设置初始权重。举例来说，对于一个单元的输入值，不管权重w，还是放缩后的权kw，BN过后的值都是一样的，<strong>这个k被消掉了</strong>，对于学习来说，激活值是一样的。</p></li></ol><h1 id="ResNet网络参数详解"><a href="#ResNet网络参数详解" class="headerlink" title="ResNet网络参数详解"></a>ResNet网络参数详解</h1><p><img src="104101.png" alt="网络结构"></p><p><strong>只演示第一个残差部分</strong></p><p>（1）输入为56×56×64，卷积核数量为64个；卷积核的尺寸大小为3×3×64；步幅为1（stride&#x3D;1），填充为1（padding&#x3D;1）；卷积后得到shape为56×56×64的特征图输出。</p><p>（2）输入为56×56×64，将输入的特征图经过批量规范化，然后经过RULE激活函数进行激活。输出的特征图大小形状不变，为56×56×64。</p><p>（3）输入为56×56×64，卷积核数量为64个；卷积核的尺寸大小为3×3×64；步幅为1（stride&#x3D;1），填充为1（padding&#x3D;1）；卷积后得到shape为56×56×64的特征图输出。</p><p>（4）输入为56×56×64，将输入的特征图经过批量规范化。输出的特征图大小形状不变，为56×56×64。</p><p>（5）此时，残差块将最初的输入特征图和经过最后一道批量规范化输出的特征图进行进行相加，得到最后</p><p>的输出特征图经过RULE激活函数进行激活。</p><p>    <img src="104101.png" width="400" align="right">(1). 输入为56×56×64，卷积核数量为64个；卷积核的尺寸大小为3×3×64；步幅为1（stride=1），填充为1（padding=1）；卷积后得到shape为56×56×64的特征图输出。<br>(2). 输入为56×56×64，将输入的特征图经过批量规范化，然后经过RULE激活函数进行激活。输出的特征图大小形状不变，为56×56×64。<br>(3). 输入为56×56×64，卷积核数量为64个；卷积核的尺寸大小为3×3×64；步幅为1（stride=1），填充为1（padding=1）；卷积后得到shape为56×56×64的特征图输出。<br>(4). 输入为56×56×64，将输入的特征图经过批量规范化。输出的特征图大小形状不变，为56×56×64。<br>(5). 此时，残差块将最初的输入特征图和经过最后一道批量规范化输出的特征图进行进行相加，得到最后的输出特征图经过RULE激活函数进行激活</p><h1 id="ResNet总结"><a href="#ResNet总结" class="headerlink" title="ResNet总结"></a>ResNet总结</h1><p>​残差网络的出现使人们摆脱了深度的束缚，大幅改善了深度神经网络中的模型退化问题，使网络层数从数十层跃升至几百上</p><p>千层，大幅提高了模型精度，通用性强适合各种类型的数据集和任务。残差块和shortcut这种优秀的设计也极大影响了后面的网</p><p>络结构发展。</p><p>​</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>tupian</title>
      <link href="/2025/10/29/tupian/"/>
      <url>/2025/10/29/tupian/</url>
      
        <content type="html"><![CDATA[<p><img src="jietu.png" alt="jietu"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>md的演示</title>
      <link href="/2025/10/28/%E4%BB%A3%E7%A0%81%E6%BC%94%E7%A4%BA/%E4%BB%A3%E7%A0%81%E5%9D%97/"/>
      <url>/2025/10/28/%E4%BB%A3%E7%A0%81%E6%BC%94%E7%A4%BA/%E4%BB%A3%E7%A0%81%E5%9D%97/</url>
      
        <content type="html"><![CDATA[<h1 id="这是一级标题"><a href="#这是一级标题" class="headerlink" title="这是一级标题"></a>这是一级标题</h1><h2 id="这是二级标题"><a href="#这是二级标题" class="headerlink" title="这是二级标题"></a>这是二级标题</h2><h3 id="这是三级标题"><a href="#这是三级标题" class="headerlink" title="这是三级标题"></a>这是三级标题</h3><p><strong>选中想要强调的文字按下Ctrl＋b</strong></p><p><del>两个波浪线可以删除</del></p><h1 id="这是代码块的演示"><a href="#这是代码块的演示" class="headerlink" title="这是代码块的演示"></a>这是代码块的演示</h1><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">function</span> <span class="title function_">a</span> (<span class="params"></span>) &#123;</span><br><span class="line">    <span class="variable language_">console</span>.<span class="title function_">log</span>(<span class="string">&quot;hello world&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="有序列表的演示"><a href="#有序列表的演示" class="headerlink" title="有序列表的演示"></a>有序列表的演示</h1><ol><li>公平</li><li>公平</li><li>还是公平</li></ol><h1 id="无序演示"><a href="#无序演示" class="headerlink" title="无序演示"></a>无序演示</h1><ul><li>加号<ul><li>还是加号</li></ul></li></ul><ul><li>减号<ul><li>还是减号</li></ul></li></ul><h1 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h1><p><a href="www.baidu.com">哈哈哈哈</a></p><h1 id="这是表格"><a href="#这是表格" class="headerlink" title="这是表格"></a>这是表格</h1><table><thead><tr><th align="left">学号</th><th align="center">姓名</th><th align="right">年龄</th></tr></thead><tbody><tr><td align="left">114514</td><td align="center">田所</td><td align="right">24</td></tr><tr><td align="left">1919810</td><td align="center">浩三</td><td align="right">25</td></tr></tbody></table><p><img src="jietu.png" alt="图片"></p><hr><p><img src="/../../../images/31e3606101f9afb4fca19d9cf53eca6ffad8638bb06423e06c242975e0242476.png" alt="picture 0"><br>shift+alt+v</p><p>就可以直接显示出来</p><center>居中就这样</center>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2025/10/27/hello-world/"/>
      <url>/2025/10/27/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
